# Testing the one thing

When I started writing automated tests 12 years ago, there wasn't much literature
or guidance on best practices around writing "good" tests. In my office, the one
bit of guidance I did get from my colleguages was fairly standard: write tests
that verify one thing. It's fairly straightforward advice that falls in line
with other coding practices, like good unit tests and single use functions.
If you're not doing that, you must be doing things wrong, right? Enter my dilema...

Suppose you're testing a feature that takes several steps to set up. This seems
straightforward enough. The steps to set up the widget belong in the test fixture,
and the steps validating that the widget is what you think it should be belong in
tests. Simple, right? Lets look again at the steps to create the widget. What happens
if any of those steps fail? The test would fail in the fixture, and execution of that
test ends. But what *really* is broken? We don't know if the widget works because we never
made it out of the fixture. There were multiple steps required to make the widget, so which one
was it that broke and why? Technically, you're testing the widget and not really the
steps to create the widget, but you need to add validation for those steps in the fixture so
that you can be confident in understanding what went wrong. The challenge is that you now
essentially have tests in your fixture! This leads to what I've dubbed "fat fixtures", or
ones that contain so much logic and validation that they really are tests in their own right.
To be fair, they're not as much tests as they are waypoints to determine if you have the
right scenario in place to run the actual tests. This is a lot of work! If you're feeling a bit
uneasy about how things look at this point, you're not alone. It doesn't *feel* right. But what
could you do?

One option would be to have shared state or "chained" tests, where one test suite is dependent on
the state generated by the execution of a previous test. This seems like a good logical choice,
but things often fall apart in the implementation. If you have a "chain" of tests and one fails,
what tells the next tests in the chain not to execute? If there's no solution for that, the rest
of your test results are likely going to be a mess of non-existent issues, and determening the signal
from the noise can be tedious.

I think the problem here is that not all problems can be broken down into the basic given/when/then
setup/execute/teardown structures, yet those are the patterns that we still try to cram all of our problems
into. While it often works, there are enough cases where it doesn't that end up requiring shameful hacks
to solve. Rather than continuing to try to hammer the square peg into the round hole, I think it's worth stepping
back and re-analyzing the problem you're actually trying to solve: testing a workflow.